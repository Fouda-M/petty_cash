'use server';
/**
 * @fileOverview Flow for getting the latest exchange rates using a tool.
 *
 * - getLatestExchangeRates - A function that invokes the flow to fetch rates.
 * - GetLatestExchangeRatesOutput - The return type for the getLatestExchangeRates function.
 */

import { ai } from '@/ai/genkit';
import { z } from 'zod';
import { fetchExchangeRatesTool } from '@/ai/tools/fetch-exchange-rates-tool';
import { 
    ExchangeRateToolOutputSchema,
    GetLatestExchangeRatesFlowOutputSchema,
    type GetLatestExchangeRatesFlowOutput as AppGetLatestExchangeRatesOutput
} from '@/ai/schemas';


// Type for the flow, now imported via AppGetLatestExchangeRatesOutput alias
export type GetLatestExchangeRatesOutput = AppGetLatestExchangeRatesOutput;


export async function getLatestExchangeRates(): Promise<GetLatestExchangeRatesOutput> {
  return getLatestExchangeRatesFlow({});
}

const getRatesPrompt = ai.definePrompt({
    name: 'getLatestExchangeRatesPrompt',
    system: 'You are an assistant that helps fetch the latest exchange rates. You MUST use the available tool for this task.',
    tools: [fetchExchangeRatesTool],
    inputSchema: z.object({}).describe("No specific input needed for this prompt"),
    outputSchema: ExchangeRateToolOutputSchema,
    prompt: 'Fetch the latest exchange rates using the available tool. Then, provide the fetched rates *only* as a JSON object conforming to the output schema. Do not include any other text, conversation, or explanation.',
});


const getLatestExchangeRatesFlow = ai.defineFlow(
  {
    name: 'getLatestExchangeRatesFlow',
    inputSchema: z.object({}), // No input for the flow itself
    outputSchema: GetLatestExchangeRatesFlowOutputSchema, // Use imported schema from ai/schemas.ts
  },
  async () => {
    console.log('getLatestExchangeRatesFlow started');
    let llmResponse;
    try {
      llmResponse = await getRatesPrompt({});
      // Detailed logging of the entire LLM response object
      console.log('Full LLM Response from getRatesPrompt:', JSON.stringify(llmResponse, null, 2));
    } catch (promptError) {
      console.error('Error calling getRatesPrompt:', promptError);
      // It's possible promptError might be an object, ensure it's stringified or message is extracted
      const errorMessage = promptError instanceof Error ? promptError.message : String(promptError);
      console.error('getRatesPrompt error details:', JSON.stringify(promptError, null, 2));
      throw new Error(`Failed to interact with the LLM for exchange rates: ${errorMessage}`);
    }

    if (llmResponse && llmResponse.output) {
        console.log('LLM response.output received:', JSON.stringify(llmResponse.output, null, 2));
        // The output from the prompt should match ExchangeRateToolOutputSchema.
        // We then ensure it's correctly typed and validated as AppExchangeRates / GetLatestExchangeRatesOutput.
        const validatedOutput = GetLatestExchangeRatesFlowOutputSchema.safeParse(llmResponse.output);
        if (validatedOutput.success) {
            console.log('Validated output successfully:', JSON.stringify(validatedOutput.data, null, 2));
            return validatedOutput.data;
        } else {
            console.error('LLM output was present but failed validation against GetLatestExchangeRatesFlowOutputSchema:', validatedOutput.error.flatten());
            console.error('Raw output that failed validation:', JSON.stringify(llmResponse.output, null, 2));
            throw new Error('Fetched exchange rates data is invalid or not in the expected format.');
        }
    } else {
        // llmResponse.output is not present or llmResponse is null/undefined
        console.error('LLM response.output was not found or llmResponse was null/undefined.');
        if (llmResponse && llmResponse.candidates && llmResponse.candidates.length > 0) {
            const candidate = llmResponse.candidates[0];
            console.error('LLM response primary candidate details:', JSON.stringify(candidate, null, 2));
            if (candidate.message && candidate.message.content) {
                let foundToolRequest = false;
                candidate.message.content.forEach(part => {
                    if (part.text) {
                        console.error('LLM text response part (instead of structured output):', part.text);
                    }
                    if (part.toolRequest) {
                        console.error('LLM generated a toolRequest:', JSON.stringify(part.toolRequest, null, 2));
                        foundToolRequest = true;
                    }
                     if (part.toolResponse) {
                        console.error('LLM part contained a toolResponse:', JSON.stringify(part.toolResponse, null, 2));
                    }
                });
                if (foundToolRequest) {
                    console.error("A toolRequest was generated by the LLM, but the final structured output (llmResponse.output) is missing. This might indicate an issue in the second LLM call (post-tool execution) or the LLM's final response didn't conform to the output schema.");
                } else {
                    console.error("No toolRequest was found in the primary candidate's message parts, and no structured output (llmResponse.output). The LLM may not have attempted to use the tool, or its response was pure text not matching the schema.");
                }
            } else {
                 console.error("Primary candidate's message or message.content was missing.");
            }
        } else if (llmResponse) {
            // Log if llmResponse exists but is not structured as expected (e.g., no candidates)
            console.error('LLM response was present but had no candidates or unexpected structure:', JSON.stringify(llmResponse, null, 2));
        } else {
            // This case means llmResponse itself was null or undefined from the getRatesPrompt call
            console.error('LLM response (from getRatesPrompt) was entirely null or undefined.');
        }
        
        throw new Error('Could not retrieve exchange rates; LLM did not provide the expected structured output (llmResponse.output was missing). Check logs for details.');
    }
  }
);
